{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF\n",
        "\n",
        "TF-IDF model is one such method to represent words in numerical values. TF-IDF stands for “Term Frequency – Inverse Document Frequency”.\n",
        "\n",
        "\n",
        "This method removes the drawbacks faced by the bag of words model. it does not assign equal value to all the words, hence important words that occur a few times will be assigned high weights."
      ],
      "metadata": {
        "id": "APWkj9Fq7IlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is the product of Term Frequency and Inverse Document Frequency. Here’s the formula for TF-IDF calculation.\n",
        "\n",
        "`TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)`\n",
        "\n",
        "\n",
        "**What is Term Frequency?**\n",
        "\n",
        "It is the measure of the frequency of words in a document. It is the ratio of the number of times the word appears in a document compared to the total number of words in that document.\n",
        "\n",
        "`tf(t,d) = count of t in d / number of words in d`\n",
        "\n",
        "\n",
        "**What is Inverse Document Frequency?**\n",
        "\n",
        "The words that occur rarely in the corpus have a high IDF score. It is the log of the ratio of the number of documents to the number of documents containing the word.\n",
        "\n",
        "We take log of this ratio because when the corpus becomes large IDF values can get large causing it to explode hence taking log will dampen this effect.\n",
        "\n",
        "we cannot divide by 0, we smoothen the value by adding 1 to the denominator.\n",
        "\n",
        "`idf(t) = log(N/(df + 1))`"
      ],
      "metadata": {
        "id": "9j1Dc6LB7UB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the data\n",
        "\n",
        "We’ll start with preprocessing the text data, and make a vocabulary set of the words in our training data and assign a unique index for each word in the set."
      ],
      "metadata": {
        "id": "xybucuag7hN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5tCYpgM567kP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import  word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlLSPRb8Fay",
        "outputId": "a79324e7-a7eb-4335-9302-90be7f39924a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text corpus for our tutorial\n",
        "\n",
        "text = ['Topic sentences are similar to mini thesis statements.\\\n",
        "        Like a thesis statement, a topic sentence has a specific \\\n",
        "        main point. Whereas the thesis is the main point of the essay',\\\n",
        "        'the topic sentence is the main point of the paragraph.\\\n",
        "        Like the thesis statement, a topic sentence has a unifying function. \\\n",
        "        But a thesis statement or topic sentence alone doesn’t guarantee unity.', \\\n",
        "        'An essay is unified if all the paragraphs relate to the thesis,\\\n",
        "        whereas a paragraph is unified if all the sentences relate to the topic sentence.']"
      ],
      "metadata": {
        "id": "H44MoXq87ru2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the text data\n",
        "sentences = []\n",
        "word_set = []"
      ],
      "metadata": {
        "id": "hYMoKgXB70Uh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in text:\n",
        "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
        "    sentences.append(x)\n",
        "    for word in x:\n",
        "        if word not in word_set:\n",
        "            word_set.append(word)"
      ],
      "metadata": {
        "id": "JaM95lj472ei"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set of vocab\n",
        "word_set = set(word_set)"
      ],
      "metadata": {
        "id": "pPiQ34En75m0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total documents in our corpus\n",
        "total_documents = len(sentences)"
      ],
      "metadata": {
        "id": "IlWx6yFf77F_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an index for each word in our vocab.\n",
        "index_dict = {} #Dictionary to store index for each word\n",
        "i = 0\n",
        "for word in word_set:\n",
        "    index_dict[word] = i\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "EtR5UBF47-Ot"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dictionary for keeping count\n",
        "\n",
        "\n",
        "We then create a dictionary to keep the count of the number of documents containing the given word."
      ],
      "metadata": {
        "id": "p0Uq2IxA8RQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a count dictionary\n",
        "\n",
        "def count_dict(sentences):\n",
        "    word_count = {}\n",
        "    for word in word_set:\n",
        "        word_count[word] = 0\n",
        "        for sent in sentences:\n",
        "            if word in sent:\n",
        "                word_count[word] += 1\n",
        "    return word_count\n",
        "\n",
        "word_count = count_dict(sentences)"
      ],
      "metadata": {
        "id": "6cXhch_C8V4L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count"
      ],
      "metadata": {
        "id": "yCKhFJuR8aTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec4d491-bab7-4c4c-c678-d4706d7e715d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 3,\n",
              " 'similar': 1,\n",
              " 'the': 3,\n",
              " 'to': 2,\n",
              " 'are': 1,\n",
              " 'thesis': 3,\n",
              " 'all': 1,\n",
              " 'topic': 3,\n",
              " 'but': 1,\n",
              " 'unified': 1,\n",
              " 'mini': 1,\n",
              " 'unifying': 1,\n",
              " 'or': 1,\n",
              " 'whereas': 2,\n",
              " 'alone': 1,\n",
              " 'paragraphs': 1,\n",
              " 'statement': 2,\n",
              " 'paragraph': 2,\n",
              " 'relate': 1,\n",
              " 'statements': 1,\n",
              " 'if': 1,\n",
              " 'main': 2,\n",
              " 'an': 1,\n",
              " 'point': 2,\n",
              " 'like': 2,\n",
              " 'unity': 1,\n",
              " 'function': 1,\n",
              " 'essay': 2,\n",
              " 't': 1,\n",
              " 'specific': 1,\n",
              " 'of': 2,\n",
              " 'sentences': 2,\n",
              " 'is': 3,\n",
              " 'a': 3,\n",
              " 'doesn': 1,\n",
              " 'guarantee': 1,\n",
              " 'has': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a function to calculate Term Frequency\n",
        "\n",
        "Define a function to count the term frequency (TF) first."
      ],
      "metadata": {
        "id": "tUC1frM-8fpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Term Frequency\n",
        "def term_freq(document, word):\n",
        "    N = len(document)\n",
        "    occurance = len([token for token in document if token == word])\n",
        "    return occurance/N"
      ],
      "metadata": {
        "id": "VJSw-W3W8kB-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a function calculate Inverse Document Frequency\n",
        "\n",
        "Define another function for the Inverse Document Frequency (IDF)\n"
      ],
      "metadata": {
        "id": "08SiXwN68qd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NhJ5jNaL85Lx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inverse Document Frequency\n",
        "def inverse_doc_freq(word):\n",
        "    try:\n",
        "        word_occurance = word_count[word] + 1\n",
        "    except:\n",
        "        word_occurance = 1\n",
        "    return np.log(total_documents/word_occurance)"
      ],
      "metadata": {
        "id": "QsgcDKfW8pmY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining the TF-IDF functions"
      ],
      "metadata": {
        "id": "VN1ngp8g8-Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(sentence):\n",
        "    tf_idf_vec = np.zeros((len(word_set),))\n",
        "    for word in sentence:\n",
        "        tf = term_freq(sentence,word)\n",
        "        idf = inverse_doc_freq(word)\n",
        "\n",
        "        value = tf*idf\n",
        "        tf_idf_vec[index_dict[word]] = value\n",
        "    return tf_idf_vec"
      ],
      "metadata": {
        "id": "zmKuBNIX9AIG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the TF-IDF Model to our text\n",
        "\n",
        "The implementation of the TF-IDF model in Python is complete. Now, let’s pass the text corpus to the function and see what the output vector looks like."
      ],
      "metadata": {
        "id": "bUa6z2es9JjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF Encoded text corpus\n",
        "vectors = []\n",
        "for sent in sentences:\n",
        "    vec = tf_idf(sent)\n",
        "    vectors.append(vec)\n",
        "index_of_thesis = index_dict['thesis']\n",
        "print(index_of_thesis)\n",
        "\n",
        "print(vectors[0][index_of_thesis])\n",
        "print(vectors[1][index_of_thesis])\n",
        "print(vectors[2][index_of_thesis])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPAzsREb9OfS",
        "outputId": "20bc6a05-44c2-460a-f263-ab48bd9ee062"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "-0.02876820724517809\n",
            "-0.017435277118289752\n",
            "-0.011064695094299266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ1fZydy9S4x",
        "outputId": "e93ee9af-6562-43f6-ab2d-1cc9c0e76b8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.0095894 ,  0.0135155 , -0.02876821,  0.        ,  0.0135155 ,\n",
              "        -0.02876821,  0.        , -0.0191788 ,  0.        ,  0.        ,\n",
              "         0.0135155 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.0135155 ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.0135155 ,\n",
              "         0.        ,  0.        , -0.0095894 , -0.02876821,  0.        ,\n",
              "         0.        ,  0.        ]),\n",
              " array([-0.02615292,  0.        , -0.03487055,  0.        ,  0.        ,\n",
              "        -0.01743528,  0.        , -0.02615292,  0.01228682,  0.        ,\n",
              "         0.        ,  0.01228682,  0.01228682,  0.        ,  0.01228682,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.01228682,  0.01228682,  0.        ,  0.01228682,  0.        ,\n",
              "         0.        ,  0.        , -0.00871764, -0.02615292,  0.01228682,\n",
              "         0.01228682,  0.        ]),\n",
              " array([-0.0110647 ,  0.        , -0.04425878,  0.        ,  0.        ,\n",
              "        -0.0110647 ,  0.03118962, -0.0110647 ,  0.        ,  0.03118962,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.01559481,  0.        ,  0.        ,  0.03118962,  0.        ,\n",
              "         0.03118962,  0.        ,  0.01559481,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        , -0.02212939, -0.0110647 ,  0.        ,\n",
              "         0.        ,  0.        ])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use in classification : Euclidean distance\n",
        "\n",
        "Euclidean distance is the shortest between the 2 points irrespective of the dimensions\n",
        "\n",
        "`dist = np.linalg.norm(point1 - point2)`"
      ],
      "metadata": {
        "id": "DzYrKsR7-R9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist_doc1_doc2 = np.linalg.norm(vectors[0] - vectors[1])\n",
        "dist_doc1_doc3 = np.linalg.norm(vectors[0] - vectors[2])\n",
        "dist_doc2_doc3 = np.linalg.norm(vectors[1] - vectors[2])\n",
        "\n",
        "print(dist_doc1_doc2 , dist_doc1_doc3, dist_doc2_doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN2fWrFm-is5",
        "outputId": "b9e98300-5fc8-4180-bd3d-89536c450842"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05261463562637202 0.07989345472253513 0.08202330182165479\n"
          ]
        }
      ]
    }
  ]
}